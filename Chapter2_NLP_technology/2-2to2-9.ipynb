{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPb+bIZNI3lkj1kpcUcXw/F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4TOD16JWU0xT","executionInfo":{"status":"ok","timestamp":1709872431724,"user_tz":-540,"elapsed":415,"user":{"displayName":"ê¹€ìˆ˜ì„±","userId":"09570314636293598107"}},"outputId":"9386a0f5-c1ac-43ee-a7eb-a83ffb3efa23"},"outputs":[{"output_type":"stream","name":"stdout","text":["n_gram = 1: [['mary'], [','], [\"n't\"], ['slap'], ['green'], ['witch'], ['.']]\n","n_gram = 2: [['mary', ','], [',', \"n't\"], [\"n't\", 'slap'], ['slap', 'green'], ['green', 'witch'], ['witch', '.']]\n","n_gram = 3: [['mary', ',', \"n't\"], [',', \"n't\", 'slap'], [\"n't\", 'slap', 'green'], ['slap', 'green', 'witch'], ['green', 'witch', '.']]\n","n_gram = 4: [['mary', ',', \"n't\", 'slap'], [',', \"n't\", 'slap', 'green'], [\"n't\", 'slap', 'green', 'witch'], ['slap', 'green', 'witch', '.']]\n","n_gram = 5: [['mary', ',', \"n't\", 'slap', 'green'], [',', \"n't\", 'slap', 'green', 'witch'], [\"n't\", 'slap', 'green', 'witch', '.']]\n","n_gram = 6: [['mary', ',', \"n't\", 'slap', 'green', 'witch'], [',', \"n't\", 'slap', 'green', 'witch', '.']]\n"]}],"source":["#2-2 Unigram, Bigram, Trigram, ... n-gram\n","\n","def n_grams(text, n):\n","  return [text[i:i + n] for i in range(len(text)- n + 1)]\n","\n","cleaned = [\"mary\", \",\", \"n't\", \"slap\", \"green\", \"witch\", \".\"]\n","\n","for i in range(1, len(cleaned)):\n","  print(\"n_gram = %d:\" % i, end=\" \")\n","  print(n_grams(cleaned, i))"]},{"cell_type":"code","source":["# 2-3 lemma(í‘œì œì–´) and stem(ì–´ê°„)\n","\n","#flow, flew, flies, flown -> flyë¼ëŠ” ì›í˜•ì„ ê°€ì§„ ë‹¨ì–´ë¡œ í†µì¼\n","#lemmaëŠ” fly, ë²¡í„° í‘œí˜„ì˜ ì°¨ì›ì„ ì¤„ì´ëŠ” íš¨ê³¼ê°€ ìˆìŒ. ...?\n","#ì–´ê°„ ë° í‘œì œì–´ ì¶”ì¶œì€ Bag of Words ë°©ì‹ì— ìì£¼ ì‚¬ìš©ë¨. -> AIê¸°ìˆ ì›ì²œì´ ì•„ë‹˜, ë”°ë¼ì„œ.. ì´ê±´ Vocabularyì˜ì°¨ì›ì„ ì¤„ì´ê² ë‹¤ëŠ” ëœ»ì„.\n","\n","import spacy\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp(u\"he was running late\")\n","for token in doc:\n","  print(\"{} -- > {}\".format(token, token.lemma_))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fm2tRT1kV2jf","executionInfo":{"status":"ok","timestamp":1709872927399,"user_tz":-540,"elapsed":7046,"user":{"displayName":"ê¹€ìˆ˜ì„±","userId":"09570314636293598107"}},"outputId":"41c90b08-e8d6-4720-f497-94b1423b1a90"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["he -- > he\n","was -- > be\n","running -- > run\n","late -- > late\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","#ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ì–´ê°„ì„ ì¶”ì¶œí•˜ëŠ” ì‘ì—… Porterì•Œê³ ë¦¬ì¦˜ ì´ë¼ê³  ë¶€ë¦„.\n","stemmer = PorterStemmer()\n","\n","sentence = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n","tokenized_sentence = word_tokenize(sentence)\n","\n","print('ì–´ê°„ ì¶”ì¶œ ì „ :', tokenized_sentence)\n","print('ì–´ê°„ ì¶”ì¶œ í›„ :',[stemmer.stem(word) for word in tokenized_sentence])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6E-B0ylSX1jC","executionInfo":{"status":"ok","timestamp":1709873144055,"user_tz":-540,"elapsed":1047,"user":{"displayName":"ê¹€ìˆ˜ì„±","userId":"09570314636293598107"}},"outputId":"9b124fa2-fd00-4038-ccd9-819be35e6e91"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["ì–´ê°„ ì¶”ì¶œ ì „ : ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n","ì–´ê°„ ì¶”ì¶œ í›„ : ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"]}]},{"cell_type":"code","source":["# 2-4 ~ 2-6\n","# 2-4 -> í’ˆì‚¬ íƒœê¹…(part-of-speech(POS) tagging)\n","\n","import spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp(u\"Wow, My mary slapped fast her head so I have to make the book.\")\n","for token in doc:\n","  print(\"{}-{}\".format(token, token.pos_))\n","\n","\"\"\"\n","***spacyì˜ pos_ì—ì„œ ì‚¬ìš©í•˜ëŠ” íƒœê·¸ë“¤***\n","\n","he docs list the following coarse-grained tags used for the pos and pos_ attributes:\n","\n","ADJ: adjective, e.g. big, old, green, incomprehensible, first -> í˜•ìš©ì‚¬\n","ADP: adposition, e.g. in, to, during -> ì „ì¹˜ì‚¬\n","ADV: adverb, e.g. very, tomorrow, down, where, there -> ë¶€ì‚¬\n","AUX: auxiliary, e.g. is, has (done), will (do), should (do) -> ì¡°ë™ì‚¬\n","CONJ: conjunction, e.g. and, or, but -> ì ‘ì†ì‚¬\n","CCONJ: coordinating conjunction, e.g. and, or, but -> ë“±ìœ„ ì ‘ì†ì‚¬\n","DET: determiner, e.g. a, an, the -> í•œì •ì‚¬(ê´€ì‚¬)\n","INTJ: interjection, e.g. psst, ouch, bravo, hello -> ê°íƒ„ì‚¬\n","NOUN: noun, e.g. girl, cat, tree, air, beauty -> ëª…ì‚¬\n","NUM: numeral, e.g. 1, 2017, one, seventy-seven, IV, MMXIV -> ìˆ«ì\n","PART: particle, e.g. â€™s, not, -> ë¶ˆë³€í™”ì‚¬ ... https://m.blog.naver.com/jaein3389/220316343318\n","PRON: pronoun, e.g I, you, he, she, myself, themselves, somebody -> ëŒ€ëª…ì‚¬\n","PROPN: proper noun, e.g. Mary, John, London, NATO, HBO -> ê³ ìœ ëª…ì‚¬(ì‚¬ëŒì´ë¦„ ê¸°ì—…ì´ë¦„, ë‹¨ì²´ì´ë¦„ ê°™ì€ ê²ƒ)\n","PUNCT: punctuation, e.g. ., (, ), ? -> ë¬¸ì¥ë¶€í˜¸\n","SCONJ: subordinating conjunction, e.g. if, while, that -> ì¢…ì† ì ‘ì†ì‚¬(ì£¼ì ˆì´ ìˆì–´ì•¼ ë¬¸ì¥ì´ ì„±ë¦½)\n","SYM: symbol, e.g. $, %, Â§, Â©, +, âˆ’, Ã—, Ã·, =, :), ğŸ˜ -> ê¸°í˜¸\n","VERB: verb, e.g. run, runs, running, eat, ate, eating -> ë™ì‚¬\n","X: other, e.g. sfpksdpsxmsa -> ê¸°íƒ€\n","SPACE: space, e.g. -> ì—¬ë°±\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":345},"id":"E5bFZWHOY31a","executionInfo":{"status":"ok","timestamp":1709873777562,"user_tz":-540,"elapsed":1227,"user":{"displayName":"ê¹€ìˆ˜ì„±","userId":"09570314636293598107"}},"outputId":"1fdcc3c7-5885-4b5d-fbc4-6f65f937a7e0"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Wow-INTJ\n",",-PUNCT\n","My-PRON\n","mary-NOUN\n","slapped-VERB\n","fast-ADV\n","her-PRON\n","head-NOUN\n","so-SCONJ\n","I-PRON\n","have-VERB\n","to-PART\n","make-VERB\n","the-DET\n","book-NOUN\n",".-PUNCT\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\nì°¸ê³ : ì˜ì–´ í’ˆì‚¬ì˜ ì¢…ë¥˜\\n1. Noun - ëª…ì‚¬\\n2. Pronoun - ëŒ€ëª…ì‚¬\\n3. Verb - ë™ì‚¬\\n4. Adjective - í˜•ìš©ì‚¬\\n5. Adverb - ë¶€ì‚¬\\n6. Conjunction - ì ‘ì†ì‚¬\\n7. Preposition -  ì „ì¹˜ì‚¬\\n8. Interjection - ê°íƒ„ì‚¬\\n9. Determiner - í•œì •ì‚¬\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# 2-6 chunking and Named Entity Recognition(ì²­í¬ ë‚˜ëˆ„ê¸°ì™€ ê°œì²´ëª… ì¸ì‹)\n","\n","#chunking - ëª…ì‚¬êµ¬ ë¶„ì„ ìˆ˜í–‰\n","import spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp(u\"Mary slapped the green witch.\")\n","for chunk in doc.noun_chunks:\n","  print(\"{}-{}\".format(chunk, chunk.label_))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uq1WcqS0dXdo","executionInfo":{"status":"ok","timestamp":1709874701819,"user_tz":-540,"elapsed":1808,"user":{"displayName":"ê¹€ìˆ˜ì„±","userId":"09570314636293598107"}},"outputId":"db887783-0cb9-4e93-8eb2-88bd4fed6df5"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Mary-NP\n","the green witch-NP\n"]}]},{"cell_type":"code","source":["#named-entity-recognition\n","#ref: https://wikidocs.net/30682\n","import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","from nltk import word_tokenize, pos_tag, ne_chunk\n","\n","sentence = \"James is working at Disney in London\"\n","# í† í°í™” í›„ í’ˆì‚¬ íƒœê¹…\n","tokenized_sentence = pos_tag(word_tokenize(sentence))\n","print(tokenized_sentence)\n","\n","ner_sentence = ne_chunk(tokenized_sentence)\n","print(ner_sentence)\n","#PERSON <- James\n","#ORGANIZATION <- DISNEY\n","#GPE <- LONDON"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YgKonAXGeX7Y","executionInfo":{"status":"ok","timestamp":1709874884691,"user_tz":-540,"elapsed":592,"user":{"displayName":"ê¹€ìˆ˜ì„±","userId":"09570314636293598107"}},"outputId":"b6add9ce-fa13-40d4-eab4-f9b2448f77b7"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["[('James', 'NNP'), ('is', 'VBZ'), ('working', 'VBG'), ('at', 'IN'), ('Disney', 'NNP'), ('in', 'IN'), ('London', 'NNP')]\n","(S\n","  (PERSON James/NNP)\n","  is/VBZ\n","  working/VBG\n","  at/IN\n","  (ORGANIZATION Disney/NNP)\n","  in/IN\n","  (GPE London/NNP))\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n"]}]}]}
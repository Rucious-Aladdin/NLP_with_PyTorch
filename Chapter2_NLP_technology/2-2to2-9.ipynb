{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPb+bIZNI3lkj1kpcUcXw/F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4TOD16JWU0xT","executionInfo":{"status":"ok","timestamp":1709872431724,"user_tz":-540,"elapsed":415,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"9386a0f5-c1ac-43ee-a7eb-a83ffb3efa23"},"outputs":[{"output_type":"stream","name":"stdout","text":["n_gram = 1: [['mary'], [','], [\"n't\"], ['slap'], ['green'], ['witch'], ['.']]\n","n_gram = 2: [['mary', ','], [',', \"n't\"], [\"n't\", 'slap'], ['slap', 'green'], ['green', 'witch'], ['witch', '.']]\n","n_gram = 3: [['mary', ',', \"n't\"], [',', \"n't\", 'slap'], [\"n't\", 'slap', 'green'], ['slap', 'green', 'witch'], ['green', 'witch', '.']]\n","n_gram = 4: [['mary', ',', \"n't\", 'slap'], [',', \"n't\", 'slap', 'green'], [\"n't\", 'slap', 'green', 'witch'], ['slap', 'green', 'witch', '.']]\n","n_gram = 5: [['mary', ',', \"n't\", 'slap', 'green'], [',', \"n't\", 'slap', 'green', 'witch'], [\"n't\", 'slap', 'green', 'witch', '.']]\n","n_gram = 6: [['mary', ',', \"n't\", 'slap', 'green', 'witch'], [',', \"n't\", 'slap', 'green', 'witch', '.']]\n"]}],"source":["#2-2 Unigram, Bigram, Trigram, ... n-gram\n","\n","def n_grams(text, n):\n","  return [text[i:i + n] for i in range(len(text)- n + 1)]\n","\n","cleaned = [\"mary\", \",\", \"n't\", \"slap\", \"green\", \"witch\", \".\"]\n","\n","for i in range(1, len(cleaned)):\n","  print(\"n_gram = %d:\" % i, end=\" \")\n","  print(n_grams(cleaned, i))"]},{"cell_type":"code","source":["# 2-3 lemma(표제어) and stem(어간)\n","\n","#flow, flew, flies, flown -> fly라는 원형을 가진 단어로 통일\n","#lemma는 fly, 벡터 표현의 차원을 줄이는 효과가 있음. ...?\n","#어간 및 표제어 추출은 Bag of Words 방식에 자주 사용됨. -> AI기술원천이 아님, 따라서.. 이건 Vocabulary의차원을 줄이겠다는 뜻임.\n","\n","import spacy\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp(u\"he was running late\")\n","for token in doc:\n","  print(\"{} -- > {}\".format(token, token.lemma_))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fm2tRT1kV2jf","executionInfo":{"status":"ok","timestamp":1709872927399,"user_tz":-540,"elapsed":7046,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"41c90b08-e8d6-4720-f497-94b1423b1a90"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["he -- > he\n","was -- > be\n","running -- > run\n","late -- > late\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","#알고리즘을 통해 어간을 추출하는 작업 Porter알고리즘 이라고 부름.\n","stemmer = PorterStemmer()\n","\n","sentence = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n","tokenized_sentence = word_tokenize(sentence)\n","\n","print('어간 추출 전 :', tokenized_sentence)\n","print('어간 추출 후 :',[stemmer.stem(word) for word in tokenized_sentence])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6E-B0ylSX1jC","executionInfo":{"status":"ok","timestamp":1709873144055,"user_tz":-540,"elapsed":1047,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"9b124fa2-fd00-4038-ccd9-819be35e6e91"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["어간 추출 전 : ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n","어간 추출 후 : ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"]}]},{"cell_type":"code","source":["# 2-4 ~ 2-6\n","# 2-4 -> 품사 태깅(part-of-speech(POS) tagging)\n","\n","import spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp(u\"Wow, My mary slapped fast her head so I have to make the book.\")\n","for token in doc:\n","  print(\"{}-{}\".format(token, token.pos_))\n","\n","\"\"\"\n","***spacy의 pos_에서 사용하는 태그들***\n","\n","he docs list the following coarse-grained tags used for the pos and pos_ attributes:\n","\n","ADJ: adjective, e.g. big, old, green, incomprehensible, first -> 형용사\n","ADP: adposition, e.g. in, to, during -> 전치사\n","ADV: adverb, e.g. very, tomorrow, down, where, there -> 부사\n","AUX: auxiliary, e.g. is, has (done), will (do), should (do) -> 조동사\n","CONJ: conjunction, e.g. and, or, but -> 접속사\n","CCONJ: coordinating conjunction, e.g. and, or, but -> 등위 접속사\n","DET: determiner, e.g. a, an, the -> 한정사(관사)\n","INTJ: interjection, e.g. psst, ouch, bravo, hello -> 감탄사\n","NOUN: noun, e.g. girl, cat, tree, air, beauty -> 명사\n","NUM: numeral, e.g. 1, 2017, one, seventy-seven, IV, MMXIV -> 숫자\n","PART: particle, e.g. ’s, not, -> 불변화사 ... https://m.blog.naver.com/jaein3389/220316343318\n","PRON: pronoun, e.g I, you, he, she, myself, themselves, somebody -> 대명사\n","PROPN: proper noun, e.g. Mary, John, London, NATO, HBO -> 고유명사(사람이름 기업이름, 단체이름 같은 것)\n","PUNCT: punctuation, e.g. ., (, ), ? -> 문장부호\n","SCONJ: subordinating conjunction, e.g. if, while, that -> 종속 접속사(주절이 있어야 문장이 성립)\n","SYM: symbol, e.g. $, %, §, ©, +, −, ×, ÷, =, :), 😝 -> 기호\n","VERB: verb, e.g. run, runs, running, eat, ate, eating -> 동사\n","X: other, e.g. sfpksdpsxmsa -> 기타\n","SPACE: space, e.g. -> 여백\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":345},"id":"E5bFZWHOY31a","executionInfo":{"status":"ok","timestamp":1709873777562,"user_tz":-540,"elapsed":1227,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"1fdcc3c7-5885-4b5d-fbc4-6f65f937a7e0"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Wow-INTJ\n",",-PUNCT\n","My-PRON\n","mary-NOUN\n","slapped-VERB\n","fast-ADV\n","her-PRON\n","head-NOUN\n","so-SCONJ\n","I-PRON\n","have-VERB\n","to-PART\n","make-VERB\n","the-DET\n","book-NOUN\n",".-PUNCT\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\n참고: 영어 품사의 종류\\n1. Noun - 명사\\n2. Pronoun - 대명사\\n3. Verb - 동사\\n4. Adjective - 형용사\\n5. Adverb - 부사\\n6. Conjunction - 접속사\\n7. Preposition -  전치사\\n8. Interjection - 감탄사\\n9. Determiner - 한정사\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# 2-6 chunking and Named Entity Recognition(청크 나누기와 개체명 인식)\n","\n","#chunking - 명사구 분석 수행\n","import spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp(u\"Mary slapped the green witch.\")\n","for chunk in doc.noun_chunks:\n","  print(\"{}-{}\".format(chunk, chunk.label_))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uq1WcqS0dXdo","executionInfo":{"status":"ok","timestamp":1709874701819,"user_tz":-540,"elapsed":1808,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"db887783-0cb9-4e93-8eb2-88bd4fed6df5"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Mary-NP\n","the green witch-NP\n"]}]},{"cell_type":"code","source":["#named-entity-recognition\n","#ref: https://wikidocs.net/30682\n","import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","from nltk import word_tokenize, pos_tag, ne_chunk\n","\n","sentence = \"James is working at Disney in London\"\n","# 토큰화 후 품사 태깅\n","tokenized_sentence = pos_tag(word_tokenize(sentence))\n","print(tokenized_sentence)\n","\n","ner_sentence = ne_chunk(tokenized_sentence)\n","print(ner_sentence)\n","#PERSON <- James\n","#ORGANIZATION <- DISNEY\n","#GPE <- LONDON"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YgKonAXGeX7Y","executionInfo":{"status":"ok","timestamp":1709874884691,"user_tz":-540,"elapsed":592,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"b6add9ce-fa13-40d4-eab4-f9b2448f77b7"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["[('James', 'NNP'), ('is', 'VBZ'), ('working', 'VBG'), ('at', 'IN'), ('Disney', 'NNP'), ('in', 'IN'), ('London', 'NNP')]\n","(S\n","  (PERSON James/NNP)\n","  is/VBZ\n","  working/VBG\n","  at/IN\n","  (ORGANIZATION Disney/NNP)\n","  in/IN\n","  (GPE London/NNP))\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n"]}]}]}